{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oAkFBgd-eDgb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "entries = os.listdir('./pictures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rwV5HPDZe8ta",
    "outputId": "59ec3ecf-fea2-4ea4-cff2-7e1db1cd4959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 2475-1BCE\n",
      "\n",
      " Directory of C:\\Users\\varun\\Desktop\\WhatsApp\n",
      "\n",
      "09-Mar-20  11:04 AM    <DIR>          .\n",
      "09-Mar-20  11:04 AM    <DIR>          ..\n",
      "08-Mar-20  08:51 PM    <DIR>          .ipynb_checkpoints\n",
      "08-Mar-20  08:06 PM         8,144,835 My_rotation_matrix.bin\n",
      "09-Mar-20  11:08 AM    <DIR>          pictures\n",
      "08-Mar-20  01:13 PM        34,408,672 seperate.html\n",
      "08-Mar-20  01:06 PM        39,336,194 seperate.ipynb\n",
      "08-Mar-20  01:26 PM             7,626 seperate.py\n",
      "08-Mar-20  09:21 PM            28,036 seperate1.ipynb\n",
      "               5 File(s)     81,925,363 bytes\n",
      "               4 Dir(s)  99,390,148,608 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4qBZeUMDeI1v",
    "outputId": "f44bcc41-fd1b-4c14-d4ad-f775a1cfbaaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6I5bsLTweDgl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# Load an color image in grayscale\n",
    "for images in entries:\n",
    "    img=mpimg.imread(path+'/'+images)\n",
    "#     print(img.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KFnteE5deDgq"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model = models.resnet18(pretrained=True)\n",
    "layer = model._modules.get('avgpool')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKxDKxpoeDgu"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "filename = path+'/IMG-20160224-WA0006.jpg'\n",
    "# load an image in PIL format\n",
    "original = load_img(filename, target_size=(224, 224))\n",
    "print('PIL image size',original.size)\n",
    "plt.imshow(original)\n",
    "plt.show()\n",
    "\n",
    "# convert the PIL image to a numpy array\n",
    "# IN PIL - image is in (width, height, channel)\n",
    "# In Numpy - image is in (height, width, channel)\n",
    "numpy_image = img_to_array(original)\n",
    "plt.imshow(np.uint8(numpy_image))\n",
    "plt.show()\n",
    "print('numpy array size',numpy_image.shape)\n",
    "\n",
    "# Convert the image / images into batch format\n",
    "# expand_dims will add an extra dimension to the data at a particular axis\n",
    "# We want the input matrix to the network to be of the form (batchsize, height, width, channels)\n",
    "# Thus we add the extra dimension to the axis 0.\n",
    "image_batch = np.expand_dims(numpy_image, axis=0)\n",
    "print('image batch size', image_batch.shape)\n",
    "plt.imshow(np.uint8(image_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TRWlHdqeDg1"
   },
   "outputs": [],
   "source": [
    "# prepare the image for the VGG model\n",
    "processed_image = vgg16.preprocess_input(image_batch.copy())\n",
    "\n",
    "# get the predicted probabilities for each class\n",
    "predictions = vgg_model.predict(processed_image)\n",
    "# print predictions\n",
    "\n",
    "# convert the probabilities to class labels\n",
    "# We will get top 5 predictions which is the default\n",
    "label = decode_predictions(predictions)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIhlaE9meDg5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "class Img2Vec():\n",
    "\n",
    "    def __init__(self, cuda=False, model='resnet-18', layer='default', layer_output_size=512):\n",
    "        \"\"\" Img2Vec\n",
    "        :param cuda: If set to True, will run forward pass on GPU\n",
    "        :param model: String name of requested model\n",
    "        :param layer: String or Int depending on model.  See more docs: https://github.com/christiansafka/img2vec.git\n",
    "        :param layer_output_size: Int depicting the output size of the requested layer\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "        self.layer_output_size = layer_output_size\n",
    "        self.model_name = model\n",
    "        \n",
    "        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        self.scaler = transforms.Scale((224, 224))\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def get_vec(self, img, tensor=False):\n",
    "        \"\"\" Get vector embedding from PIL image\n",
    "        :param img: PIL Image or list of PIL Images\n",
    "        :param tensor: If True, get_vec will return a FloatTensor instead of Numpy array\n",
    "        :returns: Numpy ndarray\n",
    "        \"\"\"\n",
    "        if type(img) == list:\n",
    "            a = [self.normalize(self.to_tensor(self.scaler(im))) for im in img]\n",
    "            images = torch.stack(a).to(self.device) \n",
    "            if self.model_name == 'alexnet':\n",
    "                my_embedding = torch.zeros(len(img), self.layer_output_size)\n",
    "            else:\n",
    "                my_embedding = torch.zeros(len(img), self.layer_output_size, 1, 1)\n",
    "\n",
    "            def copy_data(m, i, o):\n",
    "                my_embedding.copy_(o.data)\n",
    "\n",
    "            h = self.extraction_layer.register_forward_hook(copy_data)\n",
    "            h_x = self.model(images)\n",
    "            h.remove()\n",
    "\n",
    "            if tensor:\n",
    "                return my_embedding\n",
    "            else:\n",
    "                if self.model_name == 'alexnet':\n",
    "                    return my_embedding.numpy()[:, :]\n",
    "                else:\n",
    "                    print(my_embedding.numpy()[:, :, 0, 0].shape)\n",
    "                    return my_embedding.numpy()[:, :, 0, 0]\n",
    "        else:\n",
    "            image = self.normalize(self.to_tensor(self.scaler(img))).unsqueeze(0).to(self.device)\n",
    "\n",
    "            if self.model_name == 'alexnet':\n",
    "                my_embedding = torch.zeros(1, self.layer_output_size)\n",
    "            else:\n",
    "                my_embedding = torch.zeros(1, self.layer_output_size, 1, 1)\n",
    "\n",
    "            def copy_data(m, i, o):\n",
    "                my_embedding.copy_(o.data)\n",
    "\n",
    "            h = self.extraction_layer.register_forward_hook(copy_data)\n",
    "            h_x = self.model(image)\n",
    "            h.remove()\n",
    "\n",
    "            if tensor:\n",
    "                return my_embedding\n",
    "            else:\n",
    "                if self.model_name == 'alexnet':\n",
    "                    return my_embedding.numpy()[0, :]\n",
    "                else:\n",
    "                    return my_embedding.numpy()[0, :, 0, 0]\n",
    "\n",
    "    def _get_model_and_layer(self, model_name, layer):\n",
    "        \"\"\" Internal method for getting layer from model\n",
    "        :param model_name: model name such as 'resnet-18'\n",
    "        :param layer: layer as a string for resnet-18 or int for alexnet\n",
    "        :returns: pytorch model, selected layer\n",
    "        \"\"\"\n",
    "        if model_name == 'resnet-18':\n",
    "            model = models.resnet18(pretrained=True)\n",
    "            if layer == 'default':\n",
    "                layer = model._modules.get('avgpool')\n",
    "                self.layer_output_size = 512\n",
    "            else:\n",
    "                layer = model._modules.get(layer)\n",
    "\n",
    "            return model, layer\n",
    "\n",
    "        elif model_name == 'alexnet':\n",
    "            model = models.alexnet(pretrained=True)\n",
    "            if layer == 'default':\n",
    "                layer = model.classifier[-2]\n",
    "                self.layer_output_size = 4096\n",
    "            else:\n",
    "                layer = model.classifier[-layer]\n",
    "\n",
    "            return model, layer\n",
    "\n",
    "        else:\n",
    "            raise KeyError('Model %s was not found' % model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "k8Hk8VUteDg-",
    "outputId": "d6059bd4-faea-4d5c-b42f-6ba810a1f312"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-eecb12b07998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Media/WhatsApp Images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import os\n",
    "\n",
    "input_path=path\n",
    "entries = os.listdir('./Media/WhatsApp Images')\n",
    "# entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477,
     "referenced_widgets": [
      "5f1ef02805ef4c64859324d59e5b63fa",
      "020bdd9315e04cf08cae998002531e2a",
      "365d68e42e3847318fc961a7f2590a5d",
      "f10b6fab4d3b4d2f89629a7884616b6a",
      "e543608437a34034b8cbc697a9502f30",
      "be92830749304af79d91ed5d27da7a1f",
      "2ae74ef2749747cb8cf42bf0681d0fbe",
      "dae0b66c64804205b406eb961288fd7b"
     ]
    },
    "colab_type": "code",
    "id": "CjHpdw98eDhE",
    "outputId": "1f621692-90a8-4384-ab82-760002cbcfa8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\Miniconda3\\envs\\test_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:219: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "img2vec = Img2Vec()\n",
    "from PIL import Image\n",
    "# For each test image, we store the filename and vector as key, value in a dictionary\n",
    "pics = {}\n",
    "i=0\n",
    "for file in os.listdir('./pictures'):\n",
    "    filename = os.fsdecode(file)\n",
    "    img = Image.open(os.path.join('./pictures', filename))\n",
    "    vec = img2vec.get_vec(img)\n",
    "    pics[filename] = vec    \n",
    "    i=i+1\n",
    "    if(i%200==0):\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Kxfc0Os1QGW"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"My_rotation_matrix.bin\", \"wb\") as output:\n",
    "    pickle.dump(pics, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "liIuSl8C1e8I"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"My_rotation_matrix.bin\", \"rb\") as data:\n",
    "    pics = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uZ3ibvsfeZo"
   },
   "outputs": [],
   "source": [
    "vector_key=[]\n",
    "for key in pics:\n",
    "  # print(key,pics[key])\n",
    "  vector_key+=[pics[key]]\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ii3Z-8ewl6BT",
    "outputId": "fb18dc5b-52da-4842-cc76-c24822483cde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "_1Exq1m5eDh2",
    "outputId": "031d34d5-9f26-4e20-bc9a-176778e85f7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters=2 \n",
    "from sklearn.cluster import KMeans\n",
    "Kmean = KMeans(n_clusters=5)\n",
    "Kmean.fit(vector_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_xiabrYxeDh7",
    "outputId": "bcace865-a6a6-4775-feec-0e3dcc54c113"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 4, 0, 4, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0,\n",
       "       0, 4, 4, 4, 4, 0, 0, 0, 4, 3, 3, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2,\n",
       "       2, 2, 2, 2, 4, 4, 4, 4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4,\n",
       "       4, 4, 3, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4,\n",
       "       4, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kmean.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "XnBobai3eDiC",
    "outputId": "edad1cd2-2286-425d-c7c8-36036ad1be4d"
   },
   "outputs": [],
   "source": [
    "n_clusters=5 \n",
    "\n",
    "# Python program to explain os.mkdir() method \n",
    "  \n",
    "# importing os module \n",
    "import os \n",
    "  \n",
    "# Directory \n",
    "# directory = \"1\"\n",
    "  \n",
    "# Parent Directory path \n",
    "\n",
    "# os.mkdir(path)   \n",
    "# Path \n",
    "for i in range(0,5):\n",
    "    path='./pictures'\n",
    "    path = os.path.join(path, str(i)) \n",
    "    os.mkdir(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "unk0uQbrns2n",
    "outputId": "2f063830-e41e-497b-f721-305f4dcfa430"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Kmean.labels_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HUmhzVXyo97U",
    "outputId": "582afef6-fabd-48a6-9fc8-e17c3fd5b65e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 160\n"
     ]
    }
   ],
   "source": [
    "i=0;\n",
    "count=0;\n",
    "for key in pics:\n",
    "    if((pics[key]==vector_key[i]).all()):\n",
    "        count+=1;\n",
    "    i+=1;\n",
    "print(i,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Vnz1yjPeDiK"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "i=0;\n",
    "count=0;\n",
    "for key in pics:\n",
    "    path='./pictures'\n",
    "    path = path+'/'+str(Kmean.labels_[i])\n",
    "    img = Image.open(os.path.join('./pictures', key))\n",
    "    img = img.save(path+'/'+key) \n",
    "    i+=1;\n",
    "# print(i,count)for i in range(0,len(Kmean.labels_)):\n",
    "#     path = os.path.join(path, str(Kmean.labels_[i])) \n",
    "#     # os.mkdir(path) \n",
    "#     im1 = im1.save(path+) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "seperate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "020bdd9315e04cf08cae998002531e2a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ae74ef2749747cb8cf42bf0681d0fbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "365d68e42e3847318fc961a7f2590a5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be92830749304af79d91ed5d27da7a1f",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e543608437a34034b8cbc697a9502f30",
      "value": 46827520
     }
    },
    "5f1ef02805ef4c64859324d59e5b63fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_365d68e42e3847318fc961a7f2590a5d",
       "IPY_MODEL_f10b6fab4d3b4d2f89629a7884616b6a"
      ],
      "layout": "IPY_MODEL_020bdd9315e04cf08cae998002531e2a"
     }
    },
    "be92830749304af79d91ed5d27da7a1f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dae0b66c64804205b406eb961288fd7b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e543608437a34034b8cbc697a9502f30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f10b6fab4d3b4d2f89629a7884616b6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dae0b66c64804205b406eb961288fd7b",
      "placeholder": "​",
      "style": "IPY_MODEL_2ae74ef2749747cb8cf42bf0681d0fbe",
      "value": "100% 44.7M/44.7M [00:00&lt;00:00, 191MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
